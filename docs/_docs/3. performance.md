---
title: Results of performance comparison of JVM based JSON Schema Validation Implementations
permalink: /performance
layout: single
header:
  image: /assets/images/json.png
toc: true
classes: wide
---

## Introduction

The purpose of this section is to determine how quickly each validator implementation can validate JSON documents.

## Benchmarks

Each validator implementation is run through the benchmarks below.
Each benchmark uses the [Java Microbenchmark Harness][jhm] to capture meaningful performance metrics.

The first of these benchmark covers a wide range of JSON schema functionality, while the second focuses on a more
real-world example, using a small common subset of functionality, in the context of using schema validated JSON
as a serialization format.  Combined, these should give a good comparison of performance.

**Note:**
The benchmarks are run on [GitHub's own infrastructure](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners/about-github-hosted-runners). 
These may not be dedicated machines, which can influence the stability of performance results.   
{: .notice--warning}

### JSON schema test suite benchmark

This benchmark measures the average time taken to run through all _positive_ test cases in the standard
[JSON Schema Test Suite][JSON-Schema-Test-Suite]. 
Results are broken down by implementation and schema draft specification.

Each of the following graphs compares the average time it took each implementation to validate all the **positive**
test cases, with the following caveats:

**Note:**
This benchmark excludes _negative_ tests as most production use cases only see infrequent validation failures. 
As the verbosity of error information and the cost of building this information varies greatly between implementations,
we did not want the benchmark to penalise implementations for providing rich error information.
{: .notice--warning}

**Note:**
This benchmark excludes the time spent building the validator instances and parsing the JSON schema itself.
This decision was made as most production use cases allow the same validator instance to be used many times,
meaning the cost of validation is much more important than the cost of building the validator. 
{: .notice--warning}

**Note:** 
The number of test cases in the standard test suite varies between draft specification, e.g. `DRAFT 7` 
has fewer tests than draft `2020-12`.  As the benchmark measures the time taken to run through all test for a draft specification, 
comparing performance across different draft specifications can be misleading.
{: .notice--warning}

**Note:**
The graphs below exclude the `Snow` implementation, as it is orders of magnitude slower that other implementations.
(The `Snow` implementation describes itself as a _reference_ implementation).
{: .notice--warning}

<div id="ValidateCharts"></div>

### Serde benchmark

The intent of this benchmark is to provide a more real-world benchmark. A common use of JSON is as a serialization format
for a Java object model:  A Java object is serialized to JSON and this JSON is validated against the schema before being
stored or transmitted. At a later point, the JSON is read, validated and deserialized back to the Java object.
Many use cases use a very small subset of the JSON Schema features.

This benchmark measures the average time taken to serialize a [simple Java object][TestModel], including polymorphism,
to JSON and back, validating the intermediate JSON data on both legs of the journey. 

JSON (de)serialization is generally handled by [Jackson][Jackson], except where this isn't compatible with the validation implementation.
The graphs below include the round-trip time it takes Jackson to serialise and deserialise the same instance, though with no validation,
for comparison.

The serialized form is roughly 1KB of JSON, and the schema is roughly 2KB.

Rather than test every supported schema version, the benchmark covers `DRAFT 7` and  `DRAFT 2020-12`, which covers
all currently implementations, at least once.

The schema file for `DRAFT 2020-12` can be found [here][2020-schema], and for `DRAFT 7` [here][7-schema].

Each of the following graphs compares the average time it took each implementation to serialize & validate, 
then validate & deserialize the simple Java object, with the following caveats:

**Note:**
Newer schema versions are more feature rich, and this can come at a cost.
Comparison of different implementations across specification versions may be misleading. 
{: .notice--warning}

<div id="SerdeCharts"></div>

[//]: # (Chart scripts: https://www.chartjs.org/docs/latest/)
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

[//]: # (Table scripts: https://github.com/fiduswriter/Simple-DataTables)
<link href="https://cdn.jsdelivr.net/npm/simple-datatables@7.3.0/dist/style.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/simple-datatables@7.3.0" type="text/javascript"></script>

<script>
    const implData = {% include implementations.json %};

    const validateResults = {% include JsonValidateBenchmark.json %};
    const serdeResults = {% include JsonSerdeBenchmark.json %};

    function buildCharts(resultData, benchmarkType, drafts){
      const chartContainer = document.getElementById(benchmarkType + 'Charts');

      drafts.forEach(function(draft) {
        const canvas = document.createElement('canvas');
        chartContainer.append(canvas);

        let draftData = resultData.filter(r => r.benchmark.includes(draft)).sort(function(a, b) {
          return a.primaryMetric.score - b.primaryMetric.score;
        });

        let implNames = draftData.map(r => r.benchmark.substring(r.benchmark.lastIndexOf('_') + 1));
        new Chart(canvas, 
          {
            type: 'bar',
            data: {
              labels: implNames,
              datasets: [{
                data: draftData.map(r => r.primaryMetric.score),
                borderColor: implNames.map(implName => implData.find(impl => impl.shortName === implName).color),
                backgroundColor: implNames.map(implName => implData.find(impl => impl.shortName === implName).color.replace('rgb', 'rgba').replace(')', ',0.2)')),
                borderWidth: 1
             }]
            },
            options: {
              plugins: {
                  title: {
                      display: true,
                      text: draft + ' ' + benchmarkType + ' Performance (lower is better)',
                      align: 'start',
                      padding: {
                        top: 50,
                        bottom: 30
                      }
                  },
                  legend: {
                      display: false
                  }
              },
              scales: {
                y: {
                  beginAtZero: true,
                  title: {
                    display: true,
                    text: draftData[0].primaryMetric.scoreUnit
                  }
                }
              }
            },
          });
      });
  }

  buildCharts(validateResults.filter(r => !r.benchmark.includes('_Snow')), 'Validate', ["Draft_2020_12", "Draft_2019_09", "Draft_07", "Draft_06", "Draft_04", "Draft_03"]);
  buildCharts(serdeResults, 'Serde', ["Draft_2020_12", "Draft_07"]);
</script>


[JSON-Schema-Test-Suite]: https://github.com/json-schema-org/JSON-Schema-Test-Suite
[jhm]: https://github.com/openjdk/jmh
[TestModel]: https://github.com/creek-service/json-schema-validation-comparison/blob/main/src/main/java/org/creekservice/kafka/test/perf/model/ModelState.java
[Jackson]: https://github.com/FasterXML/jackson-databind
[2020-schema]: https://github.com/creek-service/json-schema-validation-comparison/blob/main/src/main/resources/schema-draft-2020-12.json
[7-schema]: https://github.com/creek-service/json-schema-validation-comparison/blob/main/src/main/resources/schema-draft-7.json